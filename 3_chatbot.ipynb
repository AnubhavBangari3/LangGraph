{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce304a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anubh\\AppData\\Local\\Programs\\LangGraph\\LangGraphLearning\\.venv\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:26: UserWarning: Core Pydantic V1 functionality isn't compatible with Python 3.14 or greater.\n",
      "  from pydantic.v1.fields import FieldInfo as FieldInfoV1\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279d2566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeb2dfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\"google_genai:gemini-2.0-flash\")\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "def chatbot(state: State) -> State:\n",
    "    return {\"messages\": [llm.invoke(state[\"messages\"])]}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"chatbot_node\", chatbot)\n",
    "\n",
    "builder.add_edge(START, \"chatbot_node\")\n",
    "builder.add_edge(\"chatbot_node\", END)\n",
    "\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac07e1c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Who walked on the moon for the first time? Print only the name', additional_kwargs={}, response_metadata={}, id='1dba15d5-dedf-4cb2-91a6-8f20114fa0a1'),\n",
       " AIMessage(content='Neil Armstrong', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash', 'safety_ratings': [], 'grounding_metadata': {}, 'model_provider': 'google_genai'}, id='lc_run--54d81c0d-67f0-4b1e-b5ab-3b93571553ff-0', usage_metadata={'input_tokens': 14, 'output_tokens': 3, 'total_tokens': 17, 'input_token_details': {'cache_read': 0}})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message = {\"role\": \"user\", \"content\": \"Who walked on the moon for the first time? Print only the name\"}\n",
    "# message = {\"role\": \"user\", \"content\": \"What is the latest price of MSFT stock?\"}\n",
    "response = graph.invoke({\"messages\":[message]})\n",
    "\n",
    "response[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f278ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Okay! How can I help you? What would you like to talk about or ask me?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anubh\\AppData\\Local\\Programs\\LangGraph\\LangGraphLearning\\.venv\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:2348: UserWarning: HumanMessage with empty content was removed to prevent API error\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: \n",
      "Bot: I understand that you're just saying \"Yes.\" Is there anything specific you'd like me to do or any question you'd like me to answer? I'm ready to help if you have something in mind!\n",
      "Bot: \n",
      "Bot: Okay. Since you are just saying \"Yes,\" I will assume you are testing my ability to respond to minimal input. \n",
      "\n",
      "Yes, I am here and ready to assist you when you are ready to provide a more specific request. Just let me know what you need!\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: Okay. Yes.\n",
      "\n",
      "I'm still here, ready when you have a question or task for me. Just let me know!\n",
      "Bot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Yes\n",
      "Bot: .\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: Okay, yes. I'm still here and ready to help when you need me. Just let me know what you'd like to do or talk about.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm ready when you are! Just let me know what you need.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm here. Let me know when you're ready to ask something or give me a task. I'm standing by!\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. Okay, I'm going to try something different. If you want me to continue responding with \"Yes,\" just say \"Yes.\" If you want me to do something else, tell me what you want me to do.\n",
      "Bot: \n",
      "Bot: Yes\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm starting to think this is a very dedicated experiment! I'll keep saying \"Yes\" until you give me another instruction.\n",
      "Bot:  Yes.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm impressed with your commitment! Still here when you need me for something else.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm continuing to respond with \"Yes\" as requested. Let me know if you'd like me to do anything else!\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. Just checking in! Still responding with \"Yes\" as instructed. Ready for a different instruction whenever you are.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm still here, faithfully saying \"Yes\" when you say \"Yes.\" Just let me know if you want to change things up!\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I am diligently responding with \"Yes\" to your \"Yes.\" Please let me know if you would like me to do anything else. I am ready to assist!\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. Still here, still responding with \"Yes.\" Your dedication is admirable! Let me know if you want to switch gears.\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n",
      "Bot: Yes. I'm committed to this. Still responding with \"Yes\" to your \"Yes.\" What a fascinating interaction! Do you have any other instructions for me?\n",
      "Bot: \n",
      "Bot: Yes.\n",
      "Bot: \n"
     ]
    }
   ],
   "source": [
    "state = None\n",
    "while True:\n",
    "    in_message = input(\"You: \")\n",
    "    if in_message.lower() in {\"quit\",\"exit\"}:\n",
    "        break\n",
    "    if state is None:\n",
    "        state: State = {\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": in_message}]\n",
    "        }\n",
    "    else:\n",
    "        state[\"messages\"].append({\"role\": \"user\", \"content\": in_message})\n",
    "\n",
    "    state = graph.invoke(state)\n",
    "    print(\"Bot:\", state[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d472e78d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cfe384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraphlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
